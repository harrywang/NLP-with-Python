{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`abcnews-small.csv` is a ~10k subset of the original 1 million dataset that can be downloaded at https://www.kaggle.com/therohk/million-headlines, which is about 60m unzipped. \n",
    "\n",
    "Running the whole dataset on a 2017 MacBook Pro:\n",
    "\n",
    "- the preprocessing step is about 3 minutes: `processed_docs = documents['headline_text'].map(preprocess)`\n",
    "- the training step using bag of words is about 5min 46s: `lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)`\n",
    "- the training step using tf-idf is about 9min 21s: `lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/abcnews-small.csv', error_bad_lines=False);\n",
    "#data = pd.read_csv('data/abcnews-date-text.csv', error_bad_lines=False);  # full dataset\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10001"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                    headline_text  index\n0  report highlights container terminal potential      0\n1                   mud crab business on the move      1\n2             sporting task force planning begins      2\n3               ama airs hospital reform concerns      3\n4    health service speaks out over patient death      4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline_text</th>\n      <th>index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>report highlights container terminal potential</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mud crab business on the move</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sporting task force planning begins</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ama airs hospital reform concerns</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>health service speaks out over patient death</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/harrywang/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "go\n"
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   original word stemmed\n0       caresses  caress\n1          flies     fli\n2           dies     die\n3          mules    mule\n4         denied    deni\n5           died     die\n6         agreed    agre\n7          owned     own\n8        humbled   humbl\n9          sized    size\n10       meeting    meet\n11       stating   state\n12       siezing    siez\n13   itemization    item\n14   sensational  sensat\n15   traditional  tradit\n16     reference   refer\n17     colonizer   colon\n18       plotted    plot",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original word</th>\n      <th>stemmed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>caresses</td>\n      <td>caress</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>flies</td>\n      <td>fli</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dies</td>\n      <td>die</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mules</td>\n      <td>mule</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>denied</td>\n      <td>deni</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>died</td>\n      <td>die</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>agreed</td>\n      <td>agre</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>owned</td>\n      <td>own</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>humbled</td>\n      <td>humbl</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sized</td>\n      <td>size</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>meeting</td>\n      <td>meet</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>stating</td>\n      <td>state</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>siezing</td>\n      <td>siez</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>itemization</td>\n      <td>item</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sensational</td>\n      <td>sensat</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>traditional</td>\n      <td>tradit</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>reference</td>\n      <td>refer</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>colonizer</td>\n      <td>colon</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>plotted</td>\n      <td>plot</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "original document: \n['charity', 'in', 'demand', 'from', 'sacked', 'meatworkers']\n\n\n tokenized and lemmatized document: \n['chariti', 'demand', 'sack', 'meatwork']\n"
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 1.4 s, sys: 28.6 ms, total: 1.43 s\nWall time: 1.9 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# for the 1 million dataset, wall time: 2min 59s\n",
    "processed_docs = documents['headline_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    [report, highlight, contain, termin, potenti]\n1                                     [crab, busi]\n2                 [sport, task, forc, plan, begin]\n3                   [air, hospit, reform, concern]\n4          [health, servic, speak, patient, death]\n5                              [hors, ride, enact]\n6                 [plan, intersect, revamp, begin]\n7                      [stormer, slaughter, shark]\n8               [england, deserv, favourit, oneil]\n9      [firefight, continu, contain, effort, near]\nName: headline_text, dtype: object"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 contain\n1 highlight\n2 potenti\n3 report\n4 termin\n5 busi\n6 crab\n7 begin\n8 forc\n9 plan\n10 sport\n"
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(277, 1), (435, 1)]"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word 277 (\"demand\") appears 1 time.\nWord 435 (\"sack\") appears 1 time.\n"
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0, 0.802436033281265), (1, 0.5967381439893285)]\n"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 1.4 s, sys: 390 ms, total: 1.79 s\nWall time: 4.78 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Topic: 0 \nWords: 0.037*\"interview\" + 0.035*\"polic\" + 0.020*\"home\" + 0.015*\"hous\" + 0.014*\"school\" + 0.013*\"brisban\" + 0.012*\"question\" + 0.011*\"plead\" + 0.010*\"mark\" + 0.010*\"sydney\"\nTopic: 1 \nWords: 0.022*\"kill\" + 0.020*\"protest\" + 0.019*\"polic\" + 0.019*\"chang\" + 0.015*\"budget\" + 0.014*\"trump\" + 0.013*\"island\" + 0.013*\"tasmanian\" + 0.013*\"speak\" + 0.011*\"urg\"\nTopic: 2 \nWords: 0.040*\"australia\" + 0.026*\"year\" + 0.019*\"elect\" + 0.016*\"servic\" + 0.015*\"bank\" + 0.014*\"council\" + 0.013*\"miss\" + 0.012*\"vote\" + 0.012*\"plan\" + 0.011*\"high\"\nTopic: 3 \nWords: 0.013*\"plan\" + 0.013*\"govern\" + 0.013*\"court\" + 0.013*\"farmer\" + 0.012*\"public\" + 0.012*\"turnbul\" + 0.012*\"million\" + 0.011*\"campaign\" + 0.011*\"hospit\" + 0.011*\"climat\"\nTopic: 4 \nWords: 0.022*\"open\" + 0.013*\"elect\" + 0.013*\"local\" + 0.012*\"make\" + 0.012*\"trump\" + 0.012*\"adelaid\" + 0.012*\"plan\" + 0.012*\"donald\" + 0.011*\"defend\" + 0.011*\"report\"\nTopic: 5 \nWords: 0.020*\"work\" + 0.017*\"talk\" + 0.016*\"melbourn\" + 0.015*\"help\" + 0.015*\"arrest\" + 0.014*\"drug\" + 0.013*\"australian\" + 0.013*\"win\" + 0.013*\"market\" + 0.012*\"countri\"\nTopic: 6 \nWords: 0.018*\"dead\" + 0.013*\"go\" + 0.013*\"claim\" + 0.013*\"lose\" + 0.011*\"industri\" + 0.010*\"crash\" + 0.010*\"award\" + 0.009*\"region\" + 0.009*\"jail\" + 0.009*\"rule\"\nTopic: 7 \nWords: 0.027*\"say\" + 0.019*\"polic\" + 0.017*\"death\" + 0.015*\"group\" + 0.014*\"australia\" + 0.014*\"charg\" + 0.013*\"court\" + 0.013*\"govern\" + 0.012*\"world\" + 0.011*\"farm\"\nTopic: 8 \nWords: 0.037*\"say\" + 0.026*\"attack\" + 0.016*\"crash\" + 0.015*\"health\" + 0.014*\"state\" + 0.012*\"call\" + 0.012*\"face\" + 0.012*\"woman\" + 0.011*\"report\" + 0.011*\"coast\"\nTopic: 9 \nWords: 0.027*\"charg\" + 0.022*\"australian\" + 0.019*\"warn\" + 0.018*\"face\" + 0.017*\"polic\" + 0.014*\"return\" + 0.013*\"sydney\" + 0.012*\"world\" + 0.012*\"centr\" + 0.012*\"final\"\n"
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 1.43 s, sys: 177 ms, total: 1.61 s\nWall time: 3.88 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Topic: 0 Word: 0.020*\"win\" + 0.017*\"australian\" + 0.017*\"tasmania\" + 0.015*\"test\" + 0.013*\"research\" + 0.013*\"look\" + 0.012*\"cricket\" + 0.011*\"launch\" + 0.010*\"port\" + 0.010*\"food\"\nTopic: 1 Word: 0.026*\"interview\" + 0.020*\"say\" + 0.019*\"woman\" + 0.016*\"charg\" + 0.015*\"final\" + 0.012*\"go\" + 0.012*\"perth\" + 0.009*\"polic\" + 0.009*\"australian\" + 0.008*\"indigen\"\nTopic: 2 Word: 0.019*\"say\" + 0.016*\"market\" + 0.016*\"polic\" + 0.014*\"time\" + 0.013*\"news\" + 0.012*\"brisban\" + 0.011*\"work\" + 0.011*\"melbourn\" + 0.011*\"monday\" + 0.010*\"report\"\nTopic: 3 Word: 0.021*\"elect\" + 0.017*\"miss\" + 0.016*\"plan\" + 0.012*\"talk\" + 0.012*\"centr\" + 0.012*\"leader\" + 0.011*\"aborigin\" + 0.010*\"violenc\" + 0.010*\"white\" + 0.010*\"sydney\"\nTopic: 4 Word: 0.024*\"interview\" + 0.020*\"kill\" + 0.014*\"attack\" + 0.013*\"coast\" + 0.012*\"power\" + 0.011*\"releas\" + 0.011*\"say\" + 0.010*\"peopl\" + 0.010*\"action\" + 0.009*\"step\"\nTopic: 5 Word: 0.017*\"school\" + 0.015*\"stand\" + 0.014*\"hour\" + 0.013*\"trial\" + 0.013*\"fiji\" + 0.012*\"court\" + 0.012*\"open\" + 0.011*\"accus\" + 0.011*\"countri\" + 0.011*\"murder\"\nTopic: 6 Word: 0.019*\"crash\" + 0.016*\"world\" + 0.012*\"adelaid\" + 0.011*\"record\" + 0.011*\"farm\" + 0.010*\"chines\" + 0.010*\"north\" + 0.009*\"queensland\" + 0.009*\"canberra\" + 0.009*\"discuss\"\nTopic: 7 Word: 0.018*\"govern\" + 0.016*\"want\" + 0.012*\"group\" + 0.012*\"child\" + 0.010*\"fund\" + 0.010*\"abus\" + 0.010*\"train\" + 0.010*\"sentenc\" + 0.010*\"year\" + 0.009*\"call\"\nTopic: 8 Word: 0.025*\"australia\" + 0.016*\"warn\" + 0.014*\"farmer\" + 0.012*\"court\" + 0.012*\"announc\" + 0.012*\"law\" + 0.011*\"open\" + 0.010*\"land\" + 0.010*\"lead\" + 0.010*\"tasmanian\"\nTopic: 9 Word: 0.020*\"trump\" + 0.018*\"death\" + 0.011*\"make\" + 0.011*\"hospit\" + 0.011*\"australian\" + 0.010*\"donald\" + 0.010*\"perth\" + 0.010*\"tuesday\" + 0.009*\"water\" + 0.009*\"royal\"\n"
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['chariti', 'demand', 'sack', 'meatwork']"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nScore: 0.6999604105949402\t \nTopic: 0.018*\"dead\" + 0.013*\"go\" + 0.013*\"claim\" + 0.013*\"lose\" + 0.011*\"industri\" + 0.010*\"crash\" + 0.010*\"award\" + 0.009*\"region\" + 0.009*\"jail\" + 0.009*\"rule\"\n\nScore: 0.033347394317388535\t \nTopic: 0.013*\"plan\" + 0.013*\"govern\" + 0.013*\"court\" + 0.013*\"farmer\" + 0.012*\"public\" + 0.012*\"turnbul\" + 0.012*\"million\" + 0.011*\"campaign\" + 0.011*\"hospit\" + 0.011*\"climat\"\n\nScore: 0.03334241360425949\t \nTopic: 0.040*\"australia\" + 0.026*\"year\" + 0.019*\"elect\" + 0.016*\"servic\" + 0.015*\"bank\" + 0.014*\"council\" + 0.013*\"miss\" + 0.012*\"vote\" + 0.012*\"plan\" + 0.011*\"high\"\n\nScore: 0.033337824046611786\t \nTopic: 0.037*\"say\" + 0.026*\"attack\" + 0.016*\"crash\" + 0.015*\"health\" + 0.014*\"state\" + 0.012*\"call\" + 0.012*\"face\" + 0.012*\"woman\" + 0.011*\"report\" + 0.011*\"coast\"\n\nScore: 0.03333580866456032\t \nTopic: 0.022*\"open\" + 0.013*\"elect\" + 0.013*\"local\" + 0.012*\"make\" + 0.012*\"trump\" + 0.012*\"adelaid\" + 0.012*\"plan\" + 0.012*\"donald\" + 0.011*\"defend\" + 0.011*\"report\"\n\nScore: 0.033335328102111816\t \nTopic: 0.027*\"charg\" + 0.022*\"australian\" + 0.019*\"warn\" + 0.018*\"face\" + 0.017*\"polic\" + 0.014*\"return\" + 0.013*\"sydney\" + 0.012*\"world\" + 0.012*\"centr\" + 0.012*\"final\"\n\nScore: 0.03333524614572525\t \nTopic: 0.027*\"say\" + 0.019*\"polic\" + 0.017*\"death\" + 0.015*\"group\" + 0.014*\"australia\" + 0.014*\"charg\" + 0.013*\"court\" + 0.013*\"govern\" + 0.012*\"world\" + 0.011*\"farm\"\n\nScore: 0.033335208892822266\t \nTopic: 0.020*\"work\" + 0.017*\"talk\" + 0.016*\"melbourn\" + 0.015*\"help\" + 0.015*\"arrest\" + 0.014*\"drug\" + 0.013*\"australian\" + 0.013*\"win\" + 0.013*\"market\" + 0.012*\"countri\"\n\nScore: 0.03333519399166107\t \nTopic: 0.037*\"interview\" + 0.035*\"polic\" + 0.020*\"home\" + 0.015*\"hous\" + 0.014*\"school\" + 0.013*\"brisban\" + 0.012*\"question\" + 0.011*\"plead\" + 0.010*\"mark\" + 0.010*\"sydney\"\n\nScore: 0.03333517909049988\t \nTopic: 0.022*\"kill\" + 0.020*\"protest\" + 0.019*\"polic\" + 0.019*\"chang\" + 0.015*\"budget\" + 0.014*\"trump\" + 0.013*\"island\" + 0.013*\"tasmanian\" + 0.013*\"speak\" + 0.011*\"urg\"\n"
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nScore: 0.6999433040618896\t \nTopic: 0.024*\"interview\" + 0.020*\"kill\" + 0.014*\"attack\" + 0.013*\"coast\" + 0.012*\"power\" + 0.011*\"releas\" + 0.011*\"say\" + 0.010*\"peopl\" + 0.010*\"action\" + 0.009*\"step\"\n\nScore: 0.0333525687456131\t \nTopic: 0.018*\"govern\" + 0.016*\"want\" + 0.012*\"group\" + 0.012*\"child\" + 0.010*\"fund\" + 0.010*\"abus\" + 0.010*\"train\" + 0.010*\"sentenc\" + 0.010*\"year\" + 0.009*\"call\"\n\nScore: 0.03334299474954605\t \nTopic: 0.026*\"interview\" + 0.020*\"say\" + 0.019*\"woman\" + 0.016*\"charg\" + 0.015*\"final\" + 0.012*\"go\" + 0.012*\"perth\" + 0.009*\"polic\" + 0.009*\"australian\" + 0.008*\"indigen\"\n\nScore: 0.033339973539114\t \nTopic: 0.020*\"trump\" + 0.018*\"death\" + 0.011*\"make\" + 0.011*\"hospit\" + 0.011*\"australian\" + 0.010*\"donald\" + 0.010*\"perth\" + 0.010*\"tuesday\" + 0.009*\"water\" + 0.009*\"royal\"\n\nScore: 0.033338554203510284\t \nTopic: 0.019*\"crash\" + 0.016*\"world\" + 0.012*\"adelaid\" + 0.011*\"record\" + 0.011*\"farm\" + 0.010*\"chines\" + 0.010*\"north\" + 0.009*\"queensland\" + 0.009*\"canberra\" + 0.009*\"discuss\"\n\nScore: 0.033337488770484924\t \nTopic: 0.017*\"school\" + 0.015*\"stand\" + 0.014*\"hour\" + 0.013*\"trial\" + 0.013*\"fiji\" + 0.012*\"court\" + 0.012*\"open\" + 0.011*\"accus\" + 0.011*\"countri\" + 0.011*\"murder\"\n\nScore: 0.03333703428506851\t \nTopic: 0.019*\"say\" + 0.016*\"market\" + 0.016*\"polic\" + 0.014*\"time\" + 0.013*\"news\" + 0.012*\"brisban\" + 0.011*\"work\" + 0.011*\"melbourn\" + 0.011*\"monday\" + 0.010*\"report\"\n\nScore: 0.0333360955119133\t \nTopic: 0.020*\"win\" + 0.017*\"australian\" + 0.017*\"tasmania\" + 0.015*\"test\" + 0.013*\"research\" + 0.013*\"look\" + 0.012*\"cricket\" + 0.011*\"launch\" + 0.010*\"port\" + 0.010*\"food\"\n\nScore: 0.033336006104946136\t \nTopic: 0.025*\"australia\" + 0.016*\"warn\" + 0.014*\"farmer\" + 0.012*\"court\" + 0.012*\"announc\" + 0.012*\"law\" + 0.011*\"open\" + 0.010*\"land\" + 0.010*\"lead\" + 0.010*\"tasmanian\"\n\nScore: 0.03333596512675285\t \nTopic: 0.021*\"elect\" + 0.017*\"miss\" + 0.016*\"plan\" + 0.012*\"talk\" + 0.012*\"centr\" + 0.012*\"leader\" + 0.011*\"aborigin\" + 0.010*\"violenc\" + 0.010*\"white\" + 0.010*\"sydney\"\n"
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Score: 0.6999323964118958\t Topic: 0.020*\"work\" + 0.017*\"talk\" + 0.016*\"melbourn\" + 0.015*\"help\" + 0.015*\"arrest\"\nScore: 0.03335041552782059\t Topic: 0.013*\"plan\" + 0.013*\"govern\" + 0.013*\"court\" + 0.013*\"farmer\" + 0.012*\"public\"\nScore: 0.03334531560540199\t Topic: 0.037*\"say\" + 0.026*\"attack\" + 0.016*\"crash\" + 0.015*\"health\" + 0.014*\"state\"\nScore: 0.0333448089659214\t Topic: 0.040*\"australia\" + 0.026*\"year\" + 0.019*\"elect\" + 0.016*\"servic\" + 0.015*\"bank\"\nScore: 0.033340469002723694\t Topic: 0.022*\"open\" + 0.013*\"elect\" + 0.013*\"local\" + 0.012*\"make\" + 0.012*\"trump\"\nScore: 0.03334016725420952\t Topic: 0.027*\"charg\" + 0.022*\"australian\" + 0.019*\"warn\" + 0.018*\"face\" + 0.017*\"polic\"\nScore: 0.0333387665450573\t Topic: 0.018*\"dead\" + 0.013*\"go\" + 0.013*\"claim\" + 0.013*\"lose\" + 0.011*\"industri\"\nScore: 0.03333800658583641\t Topic: 0.027*\"say\" + 0.019*\"polic\" + 0.017*\"death\" + 0.015*\"group\" + 0.014*\"australia\"\nScore: 0.03333493694663048\t Topic: 0.037*\"interview\" + 0.035*\"polic\" + 0.020*\"home\" + 0.015*\"hous\" + 0.014*\"school\"\nScore: 0.03333476185798645\t Topic: 0.022*\"kill\" + 0.020*\"protest\" + 0.019*\"polic\" + 0.019*\"chang\" + 0.015*\"budget\"\n"
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# random choose 10,000 about 1/100 from the whole dataset\n",
    "# no need to run this\n",
    "import random\n",
    "\n",
    "n = 1186018 # 1 million total \n",
    "s = 10000 # random 10k\n",
    "skip = sorted(random.sample(range(n), n-s))\n",
    "data_small = pd.read_csv('data/abcnews-date-text.csv', skiprows=skip, error_bad_lines=False, names=['index', 'headline_text'])\n",
    "data_small.drop('index', axis=1, inplace=True)\n",
    "data_small.to_csv('data/abcnews-small.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python_defaultSpec_1598387176193"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}