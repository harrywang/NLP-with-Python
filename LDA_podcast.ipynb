{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "podcast data: each csv is an episode and each line is a sentence from the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/podcast1.csv', error_bad_lines=False);\n",
    "documents = data[['headline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Let's face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It podcasting is a booming business.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have information that I want the world to he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well good news.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I found the answer anchor anchor dot.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline\n",
       "0                                        Let's face.\n",
       "1               It podcasting is a booming business.\n",
       "2  I have information that I want the world to he...\n",
       "3                                    Well good news.\n",
       "4              I found the answer anchor anchor dot."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rachelzheng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.01 s, sys: 88.5 ms, total: 2.1 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_docs = documents['headline'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               [face]\n",
       "1                                [podcast, boom, busi]\n",
       "2    [inform, want, world, hear, want, outrag, fee,...\n",
       "3                                         [good, news]\n",
       "4                             [answer, anchor, anchor]\n",
       "5    [anchor, creat, content, distribut, free, worl...\n",
       "6            [record, add, pay, play, listen, audienc]\n",
       "7    [record, host, guest, music, bumper, track, im...\n",
       "8    [want, creat, podcast, budget, expens, studio,...\n",
       "9    [work, match, respons, listen, audienc, pay, p...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 face\n",
      "1 boom\n",
      "2 busi\n",
      "3 podcast\n",
      "4 charg\n",
      "5 distributor\n",
      "6 fee\n",
      "7 hear\n",
      "8 inform\n",
      "9 outrag\n",
      "10 want\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this for podcast\n",
    "# filtering tokens\n",
    "#less than 15 documents (absolute number) or\n",
    "#more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "#after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 4 (\"charg\") appears 1 time.\n",
      "Word 5 (\"distributor\") appears 1 time.\n",
      "Word 6 (\"fee\") appears 1 time.\n",
      "Word 7 (\"hear\") appears 1 time.\n",
      "Word 8 (\"inform\") appears 1 time.\n",
      "Word 9 (\"outrag\") appears 1 time.\n",
      "Word 10 (\"want\") appears 2 time.\n",
      "Word 11 (\"world\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_2 = bow_corpus[2]\n",
    "\n",
    "for i in range(len(bow_doc_2)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_2[i][0], \n",
    "                                                     dictionary[bow_doc_2[i][0]], \n",
    "                                                     bow_doc_2[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 207 ms, sys: 46.9 ms, total: 254 ms\n",
      "Wall time: 332 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model by calculating the coherence score. \"Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic.\"\n",
    "\n",
    "- Detailed introduction: https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model by calculating the coherence score - we calculate c_v\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "cm = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence = cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score for our model is: 0.44\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coherence score for our model is: {np.round(coherence, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"babylonian\" + 0.020*\"king\" + 0.017*\"string\" + 0.015*\"music\" + 0.013*\"languag\" + 0.013*\"sound\" + 0.012*\"nebuchadnezzar\" + 0.012*\"imag\" + 0.012*\"worship\" + 0.012*\"second\"\n",
      "Topic: 1 \n",
      "Words: 0.020*\"string\" + 0.019*\"note\" + 0.017*\"instrument\" + 0.015*\"like\" + 0.012*\"list\" + 0.011*\"music\" + 0.011*\"look\" + 0.010*\"liar\" + 0.010*\"play\" + 0.010*\"anchor\"\n",
      "Topic: 2 \n",
      "Words: 0.022*\"podcast\" + 0.017*\"music\" + 0.015*\"broadcast\" + 0.013*\"anchor\" + 0.011*\"note\" + 0.011*\"import\" + 0.010*\"cuneiform\" + 0.010*\"worship\" + 0.010*\"record\" + 0.009*\"time\"\n",
      "Topic: 3 \n",
      "Words: 0.026*\"music\" + 0.012*\"understand\" + 0.012*\"instrument\" + 0.012*\"number\" + 0.011*\"know\" + 0.011*\"imag\" + 0.010*\"chapter\" + 0.010*\"bibl\" + 0.010*\"king\" + 0.009*\"like\"\n",
      "Topic: 4 \n",
      "Words: 0.019*\"string\" + 0.018*\"instrument\" + 0.018*\"mode\" + 0.015*\"go\" + 0.014*\"copi\" + 0.010*\"fourth\" + 0.010*\"node\" + 0.009*\"tune\" + 0.009*\"like\" + 0.008*\"today\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 249 ms, sys: 55.2 ms, total: 304 ms\n",
      "Wall time: 444 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.009*\"yeshua\" + 0.008*\"podcast\" + 0.008*\"anchor\" + 0.007*\"copi\" + 0.007*\"babylonian\" + 0.006*\"spirit\" + 0.006*\"tuna\" + 0.006*\"amen\" + 0.006*\"fourth\" + 0.006*\"shall\"\n",
      "Topic: 1 Word: 0.009*\"string\" + 0.008*\"imag\" + 0.008*\"final\" + 0.007*\"fall\" + 0.007*\"worship\" + 0.006*\"fifth\" + 0.006*\"liar\" + 0.006*\"kadosh\" + 0.006*\"music\" + 0.006*\"akaka\"\n",
      "Topic: 2 Word: 0.009*\"time\" + 0.008*\"understand\" + 0.007*\"opportun\" + 0.007*\"nation\" + 0.007*\"face\" + 0.007*\"gain\" + 0.006*\"broadcast\" + 0.006*\"donat\" + 0.006*\"radio\" + 0.006*\"scale\"\n",
      "Topic: 3 Word: 0.014*\"manuscript\" + 0.009*\"mode\" + 0.007*\"like\" + 0.007*\"matter\" + 0.007*\"generat\" + 0.006*\"joyous\" + 0.006*\"thing\" + 0.006*\"triton\" + 0.006*\"follow\" + 0.006*\"note\"\n",
      "Topic: 4 Word: 0.008*\"string\" + 0.007*\"strength\" + 0.007*\"nebuchadnezzar\" + 0.007*\"king\" + 0.007*\"look\" + 0.006*\"answer\" + 0.006*\"list\" + 0.006*\"beauti\" + 0.006*\"centuri\" + 0.006*\"think\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inform',\n",
       " 'want',\n",
       " 'world',\n",
       " 'hear',\n",
       " 'want',\n",
       " 'outrag',\n",
       " 'fee',\n",
       " 'distributor',\n",
       " 'charg']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9195331931114197\t \n",
      "Topic: 0.020*\"string\" + 0.019*\"note\" + 0.017*\"instrument\" + 0.015*\"like\" + 0.012*\"list\" + 0.011*\"music\" + 0.011*\"look\" + 0.010*\"liar\" + 0.010*\"play\" + 0.010*\"anchor\"\n",
      "\n",
      "Score: 0.02022414095699787\t \n",
      "Topic: 0.022*\"babylonian\" + 0.020*\"king\" + 0.017*\"string\" + 0.015*\"music\" + 0.013*\"languag\" + 0.013*\"sound\" + 0.012*\"nebuchadnezzar\" + 0.012*\"imag\" + 0.012*\"worship\" + 0.012*\"second\"\n",
      "\n",
      "Score: 0.02012956142425537\t \n",
      "Topic: 0.022*\"podcast\" + 0.017*\"music\" + 0.015*\"broadcast\" + 0.013*\"anchor\" + 0.011*\"note\" + 0.011*\"import\" + 0.010*\"cuneiform\" + 0.010*\"worship\" + 0.010*\"record\" + 0.009*\"time\"\n",
      "\n",
      "Score: 0.020068541169166565\t \n",
      "Topic: 0.019*\"string\" + 0.018*\"instrument\" + 0.018*\"mode\" + 0.015*\"go\" + 0.014*\"copi\" + 0.010*\"fourth\" + 0.010*\"node\" + 0.009*\"tune\" + 0.009*\"like\" + 0.008*\"today\"\n",
      "\n",
      "Score: 0.020044507458806038\t \n",
      "Topic: 0.026*\"music\" + 0.012*\"understand\" + 0.012*\"instrument\" + 0.012*\"number\" + 0.011*\"know\" + 0.011*\"imag\" + 0.010*\"chapter\" + 0.010*\"bibl\" + 0.010*\"king\" + 0.009*\"like\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9195690155029297\t \n",
      "Topic: 0.009*\"string\" + 0.008*\"imag\" + 0.008*\"final\" + 0.007*\"fall\" + 0.007*\"worship\" + 0.006*\"fifth\" + 0.006*\"liar\" + 0.006*\"kadosh\" + 0.006*\"music\" + 0.006*\"akaka\"\n",
      "\n",
      "Score: 0.020143458619713783\t \n",
      "Topic: 0.009*\"time\" + 0.008*\"understand\" + 0.007*\"opportun\" + 0.007*\"nation\" + 0.007*\"face\" + 0.007*\"gain\" + 0.006*\"broadcast\" + 0.006*\"donat\" + 0.006*\"radio\" + 0.006*\"scale\"\n",
      "\n",
      "Score: 0.020101875066757202\t \n",
      "Topic: 0.008*\"string\" + 0.007*\"strength\" + 0.007*\"nebuchadnezzar\" + 0.007*\"king\" + 0.007*\"look\" + 0.006*\"answer\" + 0.006*\"list\" + 0.006*\"beauti\" + 0.006*\"centuri\" + 0.006*\"think\"\n",
      "\n",
      "Score: 0.0200973991304636\t \n",
      "Topic: 0.009*\"yeshua\" + 0.008*\"podcast\" + 0.008*\"anchor\" + 0.007*\"copi\" + 0.007*\"babylonian\" + 0.006*\"spirit\" + 0.006*\"tuna\" + 0.006*\"amen\" + 0.006*\"fourth\" + 0.006*\"shall\"\n",
      "\n",
      "Score: 0.020088206976652145\t \n",
      "Topic: 0.014*\"manuscript\" + 0.009*\"mode\" + 0.007*\"like\" + 0.007*\"matter\" + 0.007*\"generat\" + 0.006*\"joyous\" + 0.006*\"thing\" + 0.006*\"triton\" + 0.006*\"follow\" + 0.006*\"note\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4102005362510681\t Topic: 0.022*\"podcast\" + 0.017*\"music\" + 0.015*\"broadcast\" + 0.013*\"anchor\" + 0.011*\"note\"\n",
      "Score: 0.3897190988063812\t Topic: 0.020*\"string\" + 0.019*\"note\" + 0.017*\"instrument\" + 0.015*\"like\" + 0.012*\"list\"\n",
      "Score: 0.06670007854700089\t Topic: 0.019*\"string\" + 0.018*\"instrument\" + 0.018*\"mode\" + 0.015*\"go\" + 0.014*\"copi\"\n",
      "Score: 0.0666906014084816\t Topic: 0.026*\"music\" + 0.012*\"understand\" + 0.012*\"instrument\" + 0.012*\"number\" + 0.011*\"know\"\n",
      "Score: 0.06668966263532639\t Topic: 0.022*\"babylonian\" + 0.020*\"king\" + 0.017*\"string\" + 0.015*\"music\" + 0.013*\"languag\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
