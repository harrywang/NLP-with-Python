{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "podcast data: each csv is an episode and each line is a sentence from the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/podcast1.csv', error_bad_lines=False);\n",
    "documents = data[['headline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "142"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            headline\n0                                        Let's face.\n1               It podcasting is a booming business.\n2  I have information that I want the world to he...\n3                                    Well good news.\n4              I found the answer anchor anchor dot.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Let's face.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>It podcasting is a booming business.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I have information that I want the world to he...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Well good news.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I found the answer anchor anchor dot.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "#np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/harrywang/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   original word stemmed\n0       caresses  caress\n1          flies     fli\n2           dies     die\n3          mules    mule\n4         denied    deni\n5           died     die\n6         agreed    agre\n7          owned     own\n8        humbled   humbl\n9          sized    size\n10       meeting    meet\n11       stating   state\n12       siezing    siez\n13   itemization    item\n14   sensational  sensat\n15   traditional  tradit\n16     reference   refer\n17     colonizer   colon\n18       plotted    plot",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original word</th>\n      <th>stemmed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>caresses</td>\n      <td>caress</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>flies</td>\n      <td>fli</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dies</td>\n      <td>die</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mules</td>\n      <td>mule</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>denied</td>\n      <td>deni</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>died</td>\n      <td>die</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>agreed</td>\n      <td>agre</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>owned</td>\n      <td>own</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>humbled</td>\n      <td>humbl</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sized</td>\n      <td>size</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>meeting</td>\n      <td>meet</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>stating</td>\n      <td>state</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>siezing</td>\n      <td>siez</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>itemization</td>\n      <td>item</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sensational</td>\n      <td>sensat</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>traditional</td>\n      <td>tradit</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>reference</td>\n      <td>refer</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>colonizer</td>\n      <td>colon</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>plotted</td>\n      <td>plot</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 40.7 ms, sys: 2.84 ms, total: 43.5 ms\nWall time: 71.6 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "processed_docs = documents['headline'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                                               [face]\n1                                [podcast, boom, busi]\n2    [inform, want, world, hear, want, outrag, fee,...\n3                                         [good, news]\n4                             [answer, anchor, anchor]\n5    [anchor, creat, content, distribut, free, worl...\n6            [record, add, pay, play, listen, audienc]\n7    [record, host, guest, music, bumper, track, im...\n8    [want, creat, podcast, budget, expens, studio,...\n9    [work, match, respons, listen, audienc, pay, p...\nName: headline, dtype: object"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 face\n1 boom\n2 busi\n3 podcast\n4 charg\n5 distributor\n6 fee\n7 hear\n8 inform\n9 outrag\n10 want\n"
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this for podcast\n",
    "# filtering tokens\n",
    "#less than 15 documents (absolute number) or\n",
    "#more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "#after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word 4 (\"charg\") appears 1 time.\nWord 5 (\"distributor\") appears 1 time.\nWord 6 (\"fee\") appears 1 time.\nWord 7 (\"hear\") appears 1 time.\nWord 8 (\"inform\") appears 1 time.\nWord 9 (\"outrag\") appears 1 time.\nWord 10 (\"want\") appears 2 time.\nWord 11 (\"world\") appears 1 time.\n"
    }
   ],
   "source": [
    "bow_doc_2 = bow_corpus[2]\n",
    "\n",
    "for i in range(len(bow_doc_2)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_2[i][0], \n",
    "                                                     dictionary[bow_doc_2[i][0]], \n",
    "                                                     bow_doc_2[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0, 1.0)]\n"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 128 ms, sys: 24.1 ms, total: 152 ms\nWall time: 546 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Topic: 0 \nWords: 0.018*\"music\" + 0.015*\"copi\" + 0.013*\"understand\" + 0.012*\"mode\" + 0.012*\"nebuchadnezzar\" + 0.012*\"king\" + 0.011*\"bibl\" + 0.010*\"languag\" + 0.009*\"instrument\" + 0.009*\"babylonian\"\nTopic: 1 \nWords: 0.020*\"music\" + 0.016*\"podcast\" + 0.016*\"anchor\" + 0.015*\"worship\" + 0.013*\"king\" + 0.013*\"fall\" + 0.012*\"shall\" + 0.012*\"nebuchadnezzar\" + 0.012*\"sound\" + 0.011*\"time\"\nTopic: 2 \nWords: 0.025*\"instrument\" + 0.023*\"like\" + 0.016*\"music\" + 0.011*\"scholar\" + 0.010*\"liar\" + 0.010*\"text\" + 0.010*\"guitar\" + 0.010*\"tablet\" + 0.008*\"king\" + 0.008*\"string\"\nTopic: 3 \nWords: 0.047*\"string\" + 0.020*\"number\" + 0.016*\"music\" + 0.013*\"manuscript\" + 0.012*\"second\" + 0.011*\"fourth\" + 0.010*\"come\" + 0.008*\"call\" + 0.008*\"tablet\" + 0.008*\"podcast\"\nTopic: 4 \nWords: 0.024*\"note\" + 0.014*\"mode\" + 0.013*\"tune\" + 0.010*\"music\" + 0.009*\"time\" + 0.009*\"understand\" + 0.009*\"go\" + 0.009*\"list\" + 0.009*\"seven\" + 0.009*\"worship\"\n"
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 128 ms, sys: 27.4 ms, total: 156 ms\nWall time: 473 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Topic: 0 Word: 0.009*\"time\" + 0.009*\"babylonian\" + 0.007*\"add\" + 0.006*\"gain\" + 0.006*\"face\" + 0.006*\"amen\" + 0.006*\"tablet\" + 0.006*\"note\" + 0.006*\"nebuchadnezzar\" + 0.006*\"beauti\"\nTopic: 1 Word: 0.010*\"nebuchadnezzar\" + 0.010*\"king\" + 0.009*\"imag\" + 0.008*\"understand\" + 0.007*\"worship\" + 0.007*\"nation\" + 0.007*\"holi\" + 0.007*\"answer\" + 0.007*\"string\" + 0.006*\"fifth\"\nTopic: 2 Word: 0.010*\"fourth\" + 0.009*\"anchor\" + 0.008*\"deliv\" + 0.007*\"yeshua\" + 0.006*\"string\" + 0.006*\"joyous\" + 0.006*\"tablet\" + 0.006*\"composit\" + 0.006*\"text\" + 0.005*\"hand\"\nTopic: 3 Word: 0.010*\"manuscript\" + 0.010*\"like\" + 0.008*\"note\" + 0.007*\"instrument\" + 0.007*\"triton\" + 0.006*\"generat\" + 0.006*\"serv\" + 0.006*\"worship\" + 0.005*\"fifth\" + 0.005*\"choos\"\nTopic: 4 Word: 0.008*\"kadosh\" + 0.008*\"tuna\" + 0.006*\"follow\" + 0.006*\"millennium\" + 0.006*\"final\" + 0.006*\"broadcast\" + 0.006*\"list\" + 0.005*\"listen\" + 0.005*\"mode\" + 0.005*\"like\"\n"
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['inform',\n 'want',\n 'world',\n 'hear',\n 'want',\n 'outrag',\n 'fee',\n 'distributor',\n 'charg']"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "processed_docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nScore: 0.9195420742034912\t \nTopic: 0.025*\"instrument\" + 0.023*\"like\" + 0.016*\"music\" + 0.011*\"scholar\" + 0.010*\"liar\" + 0.010*\"text\" + 0.010*\"guitar\" + 0.010*\"tablet\" + 0.008*\"king\" + 0.008*\"string\"\n\nScore: 0.020303118973970413\t \nTopic: 0.020*\"music\" + 0.016*\"podcast\" + 0.016*\"anchor\" + 0.015*\"worship\" + 0.013*\"king\" + 0.013*\"fall\" + 0.012*\"shall\" + 0.012*\"nebuchadnezzar\" + 0.012*\"sound\" + 0.011*\"time\"\n\nScore: 0.020136237144470215\t \nTopic: 0.018*\"music\" + 0.015*\"copi\" + 0.013*\"understand\" + 0.012*\"mode\" + 0.012*\"nebuchadnezzar\" + 0.012*\"king\" + 0.011*\"bibl\" + 0.010*\"languag\" + 0.009*\"instrument\" + 0.009*\"babylonian\"\n\nScore: 0.020012253895401955\t \nTopic: 0.024*\"note\" + 0.014*\"mode\" + 0.013*\"tune\" + 0.010*\"music\" + 0.009*\"time\" + 0.009*\"understand\" + 0.009*\"go\" + 0.009*\"list\" + 0.009*\"seven\" + 0.009*\"worship\"\n\nScore: 0.020006300881505013\t \nTopic: 0.047*\"string\" + 0.020*\"number\" + 0.016*\"music\" + 0.013*\"manuscript\" + 0.012*\"second\" + 0.011*\"fourth\" + 0.010*\"come\" + 0.008*\"call\" + 0.008*\"tablet\" + 0.008*\"podcast\"\n"
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nScore: 0.9197124242782593\t \nTopic: 0.008*\"kadosh\" + 0.008*\"tuna\" + 0.006*\"follow\" + 0.006*\"millennium\" + 0.006*\"final\" + 0.006*\"broadcast\" + 0.006*\"list\" + 0.005*\"listen\" + 0.005*\"mode\" + 0.005*\"like\"\n\nScore: 0.020113665610551834\t \nTopic: 0.010*\"fourth\" + 0.009*\"anchor\" + 0.008*\"deliv\" + 0.007*\"yeshua\" + 0.006*\"string\" + 0.006*\"joyous\" + 0.006*\"tablet\" + 0.006*\"composit\" + 0.006*\"text\" + 0.005*\"hand\"\n\nScore: 0.020106647163629532\t \nTopic: 0.009*\"time\" + 0.009*\"babylonian\" + 0.007*\"add\" + 0.006*\"gain\" + 0.006*\"face\" + 0.006*\"amen\" + 0.006*\"tablet\" + 0.006*\"note\" + 0.006*\"nebuchadnezzar\" + 0.006*\"beauti\"\n\nScore: 0.020052103325724602\t \nTopic: 0.010*\"nebuchadnezzar\" + 0.010*\"king\" + 0.009*\"imag\" + 0.008*\"understand\" + 0.007*\"worship\" + 0.007*\"nation\" + 0.007*\"holi\" + 0.007*\"answer\" + 0.007*\"string\" + 0.006*\"fifth\"\n\nScore: 0.020015127956867218\t \nTopic: 0.010*\"manuscript\" + 0.010*\"like\" + 0.008*\"note\" + 0.007*\"instrument\" + 0.007*\"triton\" + 0.006*\"generat\" + 0.006*\"serv\" + 0.006*\"worship\" + 0.005*\"fifth\" + 0.005*\"choos\"\n"
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Score: 0.3993603587150574\t Topic: 0.024*\"note\" + 0.014*\"mode\" + 0.013*\"tune\" + 0.010*\"music\" + 0.009*\"time\"\nScore: 0.3985576927661896\t Topic: 0.047*\"string\" + 0.020*\"number\" + 0.016*\"music\" + 0.013*\"manuscript\" + 0.012*\"second\"\nScore: 0.06868214905261993\t Topic: 0.025*\"instrument\" + 0.023*\"like\" + 0.016*\"music\" + 0.011*\"scholar\" + 0.010*\"liar\"\nScore: 0.06670212745666504\t Topic: 0.018*\"music\" + 0.015*\"copi\" + 0.013*\"understand\" + 0.012*\"mode\" + 0.012*\"nebuchadnezzar\"\nScore: 0.06669768691062927\t Topic: 0.020*\"music\" + 0.016*\"podcast\" + 0.016*\"anchor\" + 0.015*\"worship\" + 0.013*\"king\"\n"
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python_defaultSpec_1598472566473"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}